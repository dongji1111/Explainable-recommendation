{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numba\n",
    "from numba import jit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform square matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bx = cuda.blockIdx.x\n",
    "    by = cuda.blockIdx.y\n",
    "    bw = cuda.blockDim.x\n",
    "    bh = cuda.blockDim.y\n",
    "    i = tx + bx * bw\n",
    "    j = ty + by * bh\n",
    "    \n",
    "    if i < C.shape[0] and j < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[i, k] * B[k, j]\n",
    "        C[i, j] = tmp\n",
    "#     C[i, j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(4766, 20)\n",
    "b = np.random.rand(20, 10239)\n",
    "c = np.random.rand(4766, 10239)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul[(149, 320), (32, 32)](a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9095836023552692e-14"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat(np.dot(a, b) - c).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4736"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "148*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10752742,  0.18954332],\n",
       "       [ 0.39086733,  0.62962934]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.94622695,  0.05819837,  0.94800342,  0.86490435],\n",
       "       [ 0.68502893,  0.21967855,  0.59076937,  0.79494082]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def vec_inner_cuda(vec1, vec2, vec3):\n",
    "    i = cuda.grid(1)\n",
    "    if i < len(vec1):\n",
    "        vec3[i] = vec1[i] * vec2[i]\n",
    "    cuda.syncthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_1 = np.random.rand(4766, 10239)\n",
    "vec_2 = np.random.rand(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = vec_1 != 0\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 2 µs, total: 6 µs\n",
      "Wall time: 10.7 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.43096461,  0.81121297,  0.08059362, ...,  0.9552316 ,\n",
       "        0.72534923,  0.79895664])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "vec_1[vec_1 != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 2 µs, total: 7 µs\n",
      "Wall time: 13.4 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.43096461,  0.81121297,  0.08059362, ...,  0.9552316 ,\n",
       "        0.72534923,  0.79895664])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "vec_1[np.nonzero(vec_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_1 = np.random.rand(1, 20)\n",
    "vec_2 = np.random.rand(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 3 µs, total: 7 µs\n",
      "Wall time: 13.4 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "k = np.inner(vec_1, vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform square matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    m = A.shape[1]\n",
    "    \n",
    "    i = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    j = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "    \n",
    "    if i < n and j < m:\n",
    "        \n",
    "#     if i < C.shape[0] and j < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[i, k] * B[k, j]\n",
    "        C[i, j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = 4096\n",
    "NM = 4096\n",
    "K = 20\n",
    "A = np.random.rand(NN, K)\n",
    "B = np.random.rand(K, NM)\n",
    "C = np.random.rand(NN, NM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockdim = (32, 32)\n",
    "griddim = (NN//blockdim[0], NM//blockdim[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "griddim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = cuda.stream()\n",
    "\n",
    "dA = cuda.to_device(A, stream)          # to device and don't come back\n",
    "dB = cuda.to_device(B, stream)    # to device and don't come back\n",
    "dC = cuda.to_device(C, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ByteCodeSupportError",
     "evalue": "<numba.cuda.compiler.AutoJitCUDAKernel object at 0x7fd3121a5d68> does not provide its bytecode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mByteCodeSupportError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d3ec402416fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatmul\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgriddim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0mSpecialize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minvoke\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         '''\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m         \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgriddim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblockdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msharedmem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0mcfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mspecialize\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    710\u001b[0m         argtypes = tuple(\n\u001b[1;32m    711\u001b[0m             [self.typingctx.resolve_argument_type(a) for a in args])\n\u001b[0;32m--> 712\u001b[0;31m         \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, sig)\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetoptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'link'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             kernel = compile_kernel(self.py_func, argtypes,\n\u001b[0;32m--> 727\u001b[0;31m                                     **self.targetoptions)\n\u001b[0m\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margtypes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mcore\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_cuda_compiler_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile_kernel\u001b[0;34m(pyfunc, args, link, debug, inline, fastmath)\u001b[0m\n\u001b[1;32m     73\u001b[0m def compile_kernel(pyfunc, args, link, debug=False, inline=False,\n\u001b[1;32m     74\u001b[0m                    fastmath=False):\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mcres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfndesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllvm_func_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     lib, kernel = cres.target_context.prepare_cuda_kernel(cres.library, fname,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mcore\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_cuda_compiler_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile_cuda\u001b[0;34m(pyfunc, return_type, args, debug, inline)\u001b[0m\n\u001b[1;32m     62\u001b[0m                                   \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                                   \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                                   locals={})\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mlibrary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numba/compiler.py\u001b[0m in \u001b[0;36mcompile_extra\u001b[0;34m(typingctx, targetctx, func, args, return_type, flags, locals, library)\u001b[0m\n\u001b[1;32m    761\u001b[0m     pipeline = Pipeline(typingctx, targetctx, library,\n\u001b[1;32m    762\u001b[0m                         args, return_type, flags, locals)\n\u001b[0;32m--> 763\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_extra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numba/compiler.py\u001b[0m in \u001b[0;36mcompile_extra\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompile_extra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytecode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionIdentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numba/bytecode.py\u001b[0m in \u001b[0;36mfrom_function\u001b[0;34m(cls, pyfunc)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             raise errors.ByteCodeSupportError(\n\u001b[0;32m--> 301\u001b[0;31m                 \"%s does not provide its bytecode\" % func)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mByteCodeSupportError\u001b[0m: <numba.cuda.compiler.AutoJitCUDAKernel object at 0x7fd3121a5d68> does not provide its bytecode"
     ]
    }
   ],
   "source": [
    "matmul[griddim, blockdim, stream](dA, dB, dC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    NN = 4096\n",
    "    NM = 4096\n",
    "\n",
    "    A = np.zeros((NN, NM), dtype=np.float64)\n",
    "    Anew = np.zeros((NN, NM), dtype=np.float64)\n",
    "\n",
    "    n = NN\n",
    "    m = NM\n",
    "    iter_max = 1000\n",
    "\n",
    "    tol = 1.0e-6\n",
    "    error = 1.0\n",
    "\n",
    "    for j in range(n):\n",
    "        A[j, 0] = 1.0\n",
    "        Anew[j, 0] = 1.0\n",
    "\n",
    "    print(\"Jacobi relaxation Calculation: %d x %d mesh\" % (n, m))\n",
    "\n",
    "    timer = time.time()\n",
    "    iter = 0\n",
    "\n",
    "    blockdim = (32, 32)\n",
    "    griddim = (NN//blockdim[0], NM//blockdim[1])\n",
    "\n",
    "    error_grid = np.zeros_like(A)\n",
    "\n",
    "    stream = cuda.stream()\n",
    "\n",
    "    dA = cuda.to_device(A, stream)          # to device and don't come back\n",
    "    dAnew = cuda.to_device(Anew, stream)    # to device and don't come back\n",
    "    derror_grid = cuda.to_device(error_grid, stream)\n",
    "\n",
    "    while error > tol and iter < iter_max:\n",
    "        assert error_grid.dtype == np.float64\n",
    "\n",
    "        jocabi_relax_core[griddim, blockdim, stream](dA, dAnew, derror_grid)\n",
    "\n",
    "        derror_grid.to_host(stream)\n",
    "\n",
    "\n",
    "        # error_grid is available on host\n",
    "        stream.synchronize()\n",
    "\n",
    "        error = np.abs(error_grid).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from numba import cuda\n",
    "\n",
    "\n",
    "# NOTE: CUDA kernel does not return any value\n",
    "\n",
    "@cuda.jit\n",
    "def jocabi_relax_core(A, Anew, error):\n",
    "    n = A.shape[0]\n",
    "    m = A.shape[1]\n",
    "\n",
    "    j = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "    i = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    if j >= 1 and j < n - 1 and i >= 1 and i < m - 1:\n",
    "#         Anew[j, i] = 0.25 * ( A[j, i + 1] + A[j, i - 1] \\\n",
    "#                             + A[j - 1, i] + A[j + 1, i])\n",
    "#         error[j, i] = Anew[j, i] - A[j, i]\n",
    "        error[j, i] = \n",
    "\n",
    "def main():\n",
    "    NN = 4096\n",
    "    NM = 4096\n",
    "\n",
    "    A = np.zeros((NN, NM), dtype=np.float64)\n",
    "    Anew = np.zeros((NN, NM), dtype=np.float64)\n",
    "\n",
    "    n = NN\n",
    "    m = NM\n",
    "    iter_max = 1000\n",
    "\n",
    "    tol = 1.0e-6\n",
    "    error = 1.0\n",
    "\n",
    "    for j in range(n):\n",
    "        A[j, 0] = 1.0\n",
    "        Anew[j, 0] = 1.0\n",
    "\n",
    "    print(\"Jacobi relaxation Calculation: %d x %d mesh\" % (n, m))\n",
    "\n",
    "    timer = time.time()\n",
    "    iter = 0\n",
    "\n",
    "    blockdim = (32, 32)\n",
    "    griddim = (NN//blockdim[0], NM//blockdim[1])\n",
    "\n",
    "    error_grid = np.zeros_like(A)\n",
    "\n",
    "    stream = cuda.stream()\n",
    "\n",
    "    dA = cuda.to_device(A, stream)          # to device and don't come back\n",
    "    dAnew = cuda.to_device(Anew, stream)    # to device and don't come back\n",
    "    derror_grid = cuda.to_device(error_grid, stream)\n",
    "\n",
    "    while error > tol and iter < iter_max:\n",
    "        assert error_grid.dtype == np.float64\n",
    "\n",
    "        jocabi_relax_core[griddim, blockdim, stream](dA, dAnew, derror_grid)\n",
    "\n",
    "        derror_grid.to_host(stream)\n",
    "\n",
    "\n",
    "        # error_grid is available on host\n",
    "        stream.synchronize()\n",
    "\n",
    "        error = np.abs(error_grid).max()\n",
    "\n",
    "        # swap dA and dAnew\n",
    "        tmp = dA\n",
    "        dA = dAnew\n",
    "        dAnew = tmp\n",
    "\n",
    "        if iter % 100 == 0:\n",
    "            print(\"%5d, %0.6f (elapsed: %f s)\" % (iter, error, time.time()-timer))\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "    runtime = time.time() - timer\n",
    "    print(\" total: %f s\" % runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000 (elapsed: 0.483345 s)\n",
      "  100, 0.002397 (elapsed: 8.034585 s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-8ce6ddf25ed1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# swap dA and dAnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
